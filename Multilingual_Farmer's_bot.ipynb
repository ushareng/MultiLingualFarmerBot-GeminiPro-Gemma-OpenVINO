{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOzUoYDp6jzc"
      },
      "outputs": [],
      "source": [
        "!pip install torch==2.0.1\n",
        "!pip install transformers\n",
        "!pip install bitsandbytes\n",
        "!pip install peft\n",
        "!pip install accelerate\n",
        "!pip install datasets\n",
        "!pip install trl\n",
        "!pip install einops\n",
        "!pip install scipy\n",
        "!pip install --upgrade openvino-nightly"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login(\"HF_TOKEN\")"
      ],
      "metadata": {
        "id": "qTU_te7m6p7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments)\n",
        "\n",
        "file_path = \"questionsv4.csv\"\n",
        "dataset = load_dataset(\"csv\", data_files={file_path})\n",
        "model_name = \"google/gemma-2b\""
      ],
      "metadata": {
        "id": "hO2_z0lK6r__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)"
      ],
      "metadata": {
        "id": "CSGC9g5l6wzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "\n",
        "    inputs = [\" \" for i in range(len(dataset['train']['questions']))]\n",
        "    print(len(inputs))\n",
        "    EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "\n",
        "    instructions = examples[\"questions\"]\n",
        "    inputs       = inputs\n",
        "    outputs      = examples[\"answers\"]\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
        "        text = prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        #print(text)\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts}\n",
        "pass\n",
        "\n",
        "from datasets import load_dataset\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)"
      ],
      "metadata": {
        "id": "u6A5YaHA61Av"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_arguments = TrainingArguments(\n",
        "        output_dir=\"./results\",\n",
        "        bf16=True,\n",
        "        use_ipex=True,\n",
        "        no_cuda=True,\n",
        "        fp16_full_eval=False,\n",
        "    )"
      ],
      "metadata": {
        "id": "fe9IYYmb6115"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        train_dataset=dataset[\"train\"],\n",
        "        dataset_text_field=\"text\",\n",
        "        max_seq_length=512,\n",
        "        tokenizer=tokenizer,\n",
        "        args=training_arguments,\n",
        "        packing=True,\n",
        "    )\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "iQ4CLg6h63q8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_model = \"finetuned-gemma\"\n",
        "trainer.model.save_pretrained(new_model)"
      ],
      "metadata": {
        "id": "K0hIPebY66HD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import transformers\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
        "from transformers import AutoModelForSeq2SeqLM, pipeline\n",
        "from huggingface_hub import login\n",
        "import numpy as np\n",
        "\n",
        "new_model = \"tensorgirl/finetuned-gemma\"\n",
        "model = AutoModelForCausalLM.from_pretrained(new_model, trust_remote_code=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(new_model, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "generator = transformers.pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        trust_remote_code=True,\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "\n",
        "def translate(text, src_lang, tgt_lang):\n",
        "\n",
        "    translation_pipeline = pipeline(\"translation\", model=model, tokenizer=tokenizer, src_lang=src_lang, tgt_lang=tgt_lang, max_length=400, device=device)\n",
        "    result = translation_pipeline(text)\n",
        "    return result[0]['translation_text']\n",
        "\n",
        "def English(audio):\n",
        "\n",
        "    transcriber = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base.en\")\n",
        "    sr, y = audio\n",
        "    y = y.astype(np.float32)\n",
        "    y /= np.max(np.abs(y))\n",
        "\n",
        "    return transcriber({\"sampling_rate\": sr, \"raw\": y})[\"text\"]\n",
        "\n",
        "def Hindi(audio):\n",
        "\n",
        "    transcriber = pipeline(\"automatic-speech-recognition\", model=\"theainerd/Wav2Vec2-large-xlsr-hindi\")\n",
        "    sr, y = audio\n",
        "    y = y.astype(np.float32)\n",
        "    y /= np.max(np.abs(y))\n",
        "\n",
        "    text = transcriber({\"sampling_rate\":sr, \"raw\":y})[\"text\"]\n",
        "\n",
        "    return translate(text, \"hin_Deva\", \"eng_Latn\")\n",
        "\n",
        "\n",
        "def Telegu(audio):\n",
        "\n",
        "    transcriber = pipeline(\"automatic-speech-recognition\", model=\"anuragshas/wav2vec2-large-xlsr-53-telugu\")\n",
        "    sr, y = audio\n",
        "    y = y.astype(np.float32)\n",
        "    y /= np.max(np.abs(y))\n",
        "\n",
        "    text = transcriber({\"sampling_rate\":sr, \"raw\":y})[\"text\"]\n",
        "\n",
        "    return translate(text, \"tel_Telu\", \"eng_Latn\")\n",
        "\n",
        "def Tamil(audio):\n",
        "\n",
        "    transcriber = pipeline(\"automatic-speech-recognition\", model=\"Harveenchadha/vakyansh-wav2vec2-tamil-tam-250\")\n",
        "    sr, y = audio\n",
        "    y = y.astype(np.float32)\n",
        "    y /= np.max(np.abs(y))\n",
        "\n",
        "    text = transcriber({\"sampling_rate\":sr, \"raw\":y})[\"text\"]\n",
        "\n",
        "    return translate(text, \"tam_Taml\", \"eng_Latn\")\n",
        "\n",
        "def Kannada(audio):\n",
        "\n",
        "    transcriber = pipeline(\"automatic-speech-recognition\", model=\"vasista22/whisper-kannada-medium\")\n",
        "    sr, y = audio\n",
        "    y = y.astype(np.float32)\n",
        "    y /= np.max(np.abs(y))\n",
        "\n",
        "    text = transcriber({\"sampling_rate\":sr, \"raw\":y})[\"text\"]\n",
        "\n",
        "    return translate(text, \"kan_Knda\", \"eng_Latn\")\n",
        "\n",
        "def predict(audio, language):\n",
        "\n",
        "    if language == \"English\":\n",
        "        message = English(audio)\n",
        "\n",
        "    if language == \"Hindi\":\n",
        "        message = Hindi(audio)\n",
        "\n",
        "    if language == \"Telegu\":\n",
        "        message = Telegu(audio)\n",
        "\n",
        "    if language == \"Tamil\":\n",
        "        message = Tamil(audio)\n",
        "\n",
        "    if language == \"Kannada\":\n",
        "        message = Kannada(audio)\n",
        "\n",
        "    print(message)\n",
        "\n",
        "    sequences = generator(\n",
        "            message,\n",
        "            max_length=200,\n",
        "            do_sample=False,\n",
        "            top_k=10,\n",
        "            num_return_sequences=1,\n",
        "            eos_token_id=tokenizer.eos_token_id,)\n",
        "\n",
        "    answer = \"\"\n",
        "    for seq in sequences:\n",
        "            answer = answer + seq['generated_text'] + \" \"\n",
        "\n",
        "    print(answer)\n",
        "    if language == \"English\":\n",
        "        return answer\n",
        "\n",
        "    if language == \"Hindi\":\n",
        "        return translate(answer,\"eng_Latn\", \"hin_Deva\")\n",
        "\n",
        "    if language == \"Telegu\":\n",
        "        return translate(answer,\"eng_Latn\", \"tel_Telu\")\n",
        "\n",
        "    if language == \"Tamil\":\n",
        "        return translate(answer, \"eng_Latn\", \"tam_Taml\")\n",
        "\n",
        "    if language == \"Kannada\":\n",
        "        return translate(answer, \"eng_Latn\", \"kan_Knda\")\n",
        "\n",
        "    return answer"
      ],
      "metadata": {
        "id": "EFMhMKoL67uL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, pipeline\n",
        "from optimum.intel import OVModelForCausalLM, OVWeightQuantizationConfig\n",
        "\n",
        "model_id = \"tensorgirl/finetuned-gemma\"\n",
        "\n",
        "# Create the quantization configuration with desired quantization parameters\n",
        "q_config = OVWeightQuantizationConfig(bits=4, group_size=128, ratio=0.8)\n",
        "\n",
        "# Create OpenVINO configuration with optimal settings for this model\n",
        "ov_config = {\"PERFORMANCE_HINT\": \"LATENCY\", \"CACHE_DIR\": \"model_cache\", \"INFERENCE_PRECISION_HINT\": \"f32\"}\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = OVModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    export=True,\n",
        "    quantization_config=q_config,\n",
        "    device=device,\n",
        "    ov_config=ov_config,\n",
        "  )"
      ],
      "metadata": {
        "id": "NyzmSUJP7DnG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}